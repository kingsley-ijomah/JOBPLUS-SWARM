# gluster_playbook.yml

# GlusterFS is a scalable, distributed file system that aggregates disk storage
# resources from multiple servers into a single global namespace. It's designed
# to handle large amounts of data and provide high-availability and performance.

# GlusterFS works by linking together multiple storage bricks over a network to create
# a unified file system. Each brick is a standard file system directory, and GlusterFS
# uses a client-server model where data is stored on servers but accessible from any
# client connected to the network.

# This setup allows for easy scaling, as new storage
# bricks can be added to the cluster without disrupting service. GlusterFS is ideal
# for cloud computing, streaming media services, and content delivery networks due to
# its flexibility and performance.
---
- name: GlusterFS Install & Setup
  hosts: servers
  become: yes
  tasks:
    - name: Install software-properties-common
      apt:
        name: software-properties-common
        state: latest
      tags:
        - proper_common

    - name: Add GlusterFS PPA
      apt_repository:
        repo: ppa:gluster/glusterfs-11
        update_cache: yes
      tags:
        - add_glusterfs_ppa

    - name: Install GlusterFS server
      apt:
        name: glusterfs-server
        state: latest
      tags:
        - install_glusterfs

    - name: Start GlusterFS service
      systemd:
        name: glusterd
        state: started
        enabled: yes
      tags:
        - start_glusterd

    - name: Enable GlusterFS service to start on boot
      systemd:
        name: glusterd
        enabled: yes
      tags:
        - enable_glusterd

    - name: Check GlusterFS service status
      command: systemctl status glusterd
      register: glusterd_status
      tags:
        - status_glusterd

    - name: Display GlusterFS service status
      debug:
        var: glusterd_status.stdout_lines
      tags:
        - status_glusterd

    - name: Set hostname to match inventory name
      hostname:
        name: '{{ inventory_hostname }}'
      when: inventory_hostname in groups['servers']
      tags:
        - set_hostname

    - name: Update /etc/hosts with localhost
      lineinfile:
        path: /etc/hosts
        line: '127.0.0.1 {{ inventory_hostname }}'
        regexp: '^127\.0\.0\.1 {{ inventory_hostname }}'
        state: present
      tags:
        - update_hosts_local

    - name: Update /etc/hosts with node IP and name
      lineinfile:
        path: /etc/hosts
        line: '{{ hostvars[item].ansible_host }} {{ item }}'
        regexp: '^{{ hostvars[item].ansible_host }} {{ item }}'
        state: present
      loop: "{{ groups['servers'] }}"
      when: hostvars[item].ansible_host is defined
      tags:
        - update_hosts_other

    - name: Create shared storage brick directory
      file:
        path: /data/bricks/shared_storage
        state: directory
        owner: root
        group: root
        mode: 0755
      tags:
        - create_brick_dir

    - name: Create mount point for shared storage
      file:
        path: /mnt/shared_storage
        state: directory
        owner: root
        group: root
        mode: 0755
      tags:
        - create_mount_point

    - name: Get Gluster peer status
      command: gluster peer status
      register: gluster_peer_status
      ignore_errors: yes
      tags:
        - get_peer_status

    - name: Set fact for existing peers hostnames
      set_fact:
        existing_peers_hostnames: "{{ gluster_peer_status.stdout | regex_findall('Hostname: (\\S+)') }}"
      when: gluster_peer_status.rc == 0
      tags:
        - set_peer_fact

    - name: Probe other peers to form a trusted pool
      gluster.gluster.gluster_peer:
        state: present
        nodes: "{{ (inventory_hostname == groups['servers'][0] and 'localhost' in (existing_peers_hostnames | default([]))) | ternary('localhost', item) }}"
      loop: "{{ groups['servers'] }}"
      when:
        - inventory_hostname != item
        - item not in existing_peers_hostnames
      tags:
        - probe_peers

# This playbook is designed to create a GlusterFS shared storage volume
# on the manager node of a swarm cluster. Since GlusterFS configuration
# and volume creation commands need only to be executed once and will
# apply across the cluster, we target only the manager node for these operations.
# This approach avoids redundant execution and ensures centralized management
# of the GlusterFS volume.

- name: Create Shared Storage Volume
  hosts: manager1
  become: yes
  tasks:
    - name: Calculate the number of replicas
      set_fact:
        replica_count: "{{ groups['servers'] | length }}"
      tags:
        - replica_count

    - name: Create GlusterFS volume named shared_storage
      command: >
        gluster volume create shared_storage replica {{ replica_count }}
        {% for host in groups['servers'] %}
        {{ host }}:/data/bricks/shared_storage
        {% endfor %}
        force
      args:
        creates: /var/lib/glusterd/vols/shared_storage
      tags:
        - create_volume

    - name: Check GlusterFS volume status
      command: gluster volume status shared_storage
      register: volume_status
      failed_when: "'Volume shared_storage is not started' not in volume_status.stderr and volume_status.rc != 0"
      tags:
        - volume_status

    - name: Start GlusterFS volume if not started
      command: gluster volume start shared_storage
      when: "'Volume shared_storage is not started' in volume_status.stderr"
      tags:
        - 
        
- name: Mount GlusterFS volume
  hosts: servers
  become: yes
  tasks:
    - name: Check if GlusterFS volume is already mounted
      command: mount | grep /mnt/shared_storage
      register: mount_check
      failed_when: mount_check.rc == 2
      changed_when: false
      ignore_errors: true
      tags:
        - check_volume_mounted

    - name: Mount GlusterFS volume
      mount:
        path: /mnt/shared_storage
        src: '{{ inventory_hostname }}:/shared_storage'
        fstype: glusterfs
        opts: defaults,_netdev
        state: mounted
      when: mount_check.rc != 0
      tags:
        - mount_volume